# ia_using_other_llm


**How to load LLM (falcon, llama2, mistral, mixtral, orca-mini...etc) on a local machine e.g Mac, PC with LM Studio and Ollama**



[Read more on flaven.fr: https://flaven.fr/2023/12/empower-your-workflow-harnessing-the-power-of-lm-studio-and-ollama-for-seamless-local-llm-execution/](https://flaven.fr/2023/12/empower-your-workflow-harnessing-the-power-of-lm-studio-and-ollama-for-seamless-local-llm-execution/)




## EXPLANATIONS
- Modelfile_advanced_llama2: Prompt Model for Ollama
- Modelfile_llama2: Prompt Model for Ollama
- Modelfile_orca-mini: Prompt Model for Ollama
- README.md: This readme
- lm_studio_try_1.png: Below a screen capture from a prompt made in LM Studio 
- math_problem_arthur.py: Python conversion from a basic mathematical problem made with Mistral
- prompts_mistral_public.md: Some prompts made on Mistral both for teenagers problems and professional issues.
- samwit_basic.py: extract
- samwit_basic_chain.py
- samwit_rag.py

**Convert a UAT made with Cypress into a gherkin to enable migration to another testing framework e.g PlayWright (https://playwright.dev) or simply the Q/A workchain**
[![(lm_studio_try_1.png) ](fastapi_postman_remember_1.png)](https://flaven.fr/)


## VIDEOS

Coming soon
