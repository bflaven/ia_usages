{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USECASE_1 : sentiment analysis of comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Source:** `sentiment_analysis_reviews_0.csv`, sample comments (568454).\n",
    "- **Objective:** to provide a sentiment analysis on all of these comments. Negative/positive, prediction via ML on the nature of the comment\n",
    "\n",
    "\n",
    "- **Note:** For educational purpose and du to limitation on GitHub, I have reduced the dataset to 1000 rows. The dataset came from this file at https://www.kaggle.com/code/robikscube/sentiment-analysis-python-youtube-tutorial/input\n",
    "\n",
    "```bash\n",
    "# the complete dataset\n",
    "[568454 rows x 10 columns]\n",
    "\n",
    "# columns\n",
    "'Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
    "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Id   ProductId          UserId                      ProfileName  \\\n",
      "0       1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
      "1       2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
      "2       3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
      "3       4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
      "4       5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
      "..    ...         ...             ...                              ...   \n",
      "995   996  B006F2NYI2  A1D3F6UI1RTXO0                           Swopes   \n",
      "996   997  B006F2NYI2   AF50D40Y85TV3                          Mike A.   \n",
      "997   998  B006F2NYI2  A3G313KLWDG3PW                          kefka82   \n",
      "998   999  B006F2NYI2  A3NIDDT7E7JIFW                  V. B. Brookshaw   \n",
      "999  1000  B006F2NYI2  A132DJVI37RB4X                        Scottdrum   \n",
      "\n",
      "     HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "0                       1                       1      5  1303862400   \n",
      "1                       0                       0      1  1346976000   \n",
      "2                       1                       1      4  1219017600   \n",
      "3                       3                       3      2  1307923200   \n",
      "4                       0                       0      5  1350777600   \n",
      "..                    ...                     ...    ...         ...   \n",
      "995                     1                       1      5  1331856000   \n",
      "996                     1                       1      5  1328140800   \n",
      "997                     1                       1      5  1324252800   \n",
      "998                     1                       2      1  1336089600   \n",
      "999                     2                       5      2  1332374400   \n",
      "\n",
      "                                    Summary  \\\n",
      "0                     Good Quality Dog Food   \n",
      "1                         Not as Advertised   \n",
      "2                     \"Delight\" says it all   \n",
      "3                            Cough Medicine   \n",
      "4                               Great taffy   \n",
      "..                                      ...   \n",
      "995                         Hot & Flavorful   \n",
      "996  Great Hot Sauce and people who run it!   \n",
      "997               this sauce is the shiznit   \n",
      "998                                 Not Hot   \n",
      "999                   Not hot, not habanero   \n",
      "\n",
      "                                                  Text  \n",
      "0    I have bought several of the Vitality canned d...  \n",
      "1    Product arrived labeled as Jumbo Salted Peanut...  \n",
      "2    This is a confection that has been around a fe...  \n",
      "3    If you are looking for the secret ingredient i...  \n",
      "4    Great taffy at a great price.  There was a wid...  \n",
      "..                                                 ...  \n",
      "995  BLACK MARKET HOT SAUCE IS WONDERFUL.... My hus...  \n",
      "996  Man what can i say, this salsa is the bomb!! i...  \n",
      "997  this sauce is so good with just about anything...  \n",
      "998  Not hot at all. Like the other low star review...  \n",
      "999  I have to admit, I was a sucker for the large ...  \n",
      "\n",
      "[1000 rows x 10 columns]\n",
      "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
      "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "CSV_SOURCE=\"data_source/sentiment_analysis_reviews_0.csv\"\n",
    "# Load the dataframe from CSV\n",
    "df = pd.read_csv(CSV_SOURCE)\n",
    "\n",
    "# show content\n",
    "print(df)\n",
    "\n",
    "# show columns\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the preparation phase, it is often necessary to reduce the size of the files, everything is explained in the file `001_split_files.py`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Id   ProductId          UserId                      ProfileName  \\\n",
      "0       1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
      "1       2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
      "2       3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
      "3       4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
      "4       5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
      "..    ...         ...             ...                              ...   \n",
      "995   996  B006F2NYI2  A1D3F6UI1RTXO0                           Swopes   \n",
      "996   997  B006F2NYI2   AF50D40Y85TV3                          Mike A.   \n",
      "997   998  B006F2NYI2  A3G313KLWDG3PW                          kefka82   \n",
      "998   999  B006F2NYI2  A3NIDDT7E7JIFW                  V. B. Brookshaw   \n",
      "999  1000  B006F2NYI2  A132DJVI37RB4X                        Scottdrum   \n",
      "\n",
      "     HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "0                       1                       1      5  1303862400   \n",
      "1                       0                       0      1  1346976000   \n",
      "2                       1                       1      4  1219017600   \n",
      "3                       3                       3      2  1307923200   \n",
      "4                       0                       0      5  1350777600   \n",
      "..                    ...                     ...    ...         ...   \n",
      "995                     1                       1      5  1331856000   \n",
      "996                     1                       1      5  1328140800   \n",
      "997                     1                       1      5  1324252800   \n",
      "998                     1                       2      1  1336089600   \n",
      "999                     2                       5      2  1332374400   \n",
      "\n",
      "                                    Summary  \\\n",
      "0                     Good Quality Dog Food   \n",
      "1                         Not as Advertised   \n",
      "2                     \"Delight\" says it all   \n",
      "3                            Cough Medicine   \n",
      "4                               Great taffy   \n",
      "..                                      ...   \n",
      "995                         Hot & Flavorful   \n",
      "996  Great Hot Sauce and people who run it!   \n",
      "997               this sauce is the shiznit   \n",
      "998                                 Not Hot   \n",
      "999                   Not hot, not habanero   \n",
      "\n",
      "                                                  Text  \n",
      "0    I have bought several of the Vitality canned d...  \n",
      "1    Product arrived labeled as Jumbo Salted Peanut...  \n",
      "2    This is a confection that has been around a fe...  \n",
      "3    If you are looking for the secret ingredient i...  \n",
      "4    Great taffy at a great price.  There was a wid...  \n",
      "..                                                 ...  \n",
      "995  BLACK MARKET HOT SAUCE IS WONDERFUL.... My hus...  \n",
      "996  Man what can i say, this salsa is the bomb!! i...  \n",
      "997  this sauce is so good with just about anything...  \n",
      "998  Not hot at all. Like the other low star review...  \n",
      "999  I have to admit, I was a sucker for the large ...  \n",
      "\n",
      "[1000 rows x 10 columns]\n",
      "the file data_split/sentiment_analysis_reviews_sample_1.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_2.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_3.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_4.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_5.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_6.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_7.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_8.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_9.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_10.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_11.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_12.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_13.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_14.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_15.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_16.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_17.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_18.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_19.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_20.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_21.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_22.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_23.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_24.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_25.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_26.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_27.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_28.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_29.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_30.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_31.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_32.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_33.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_34.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_35.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_36.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_37.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_38.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_39.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_40.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_41.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_42.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_43.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_44.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_45.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_46.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_47.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_48.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_49.csv has been created\n",
      "the file data_split/sentiment_analysis_reviews_sample_50.csv has been created\n",
      "\n",
      "--- DONE\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "[env]\n",
    "# Conda Environment\n",
    "conda create --name sentiment_analysis python=3.9.13\n",
    "conda info --envs\n",
    "source activate sentiment_analysis\n",
    "conda deactivate\n",
    "\n",
    "# if needed to remove\n",
    "conda env remove -n [NAME_OF_THE_CONDA_ENVIRONMENT]\n",
    "\n",
    "# update conda \n",
    "conda update -n base -c defaults conda\n",
    "\n",
    "# to export requirements\n",
    "pip freeze > requirements.txt\n",
    "\n",
    "# to install\n",
    "pip install -r requirements.txt\n",
    "\n",
    "\n",
    "# [path]\n",
    "cd /Users/brunoflaven/Documents/01_work/blog_articles/ia_llms_usecases/usecase_1_sentiment_analysis/\n",
    "\n",
    "# LAUNCH the file\n",
    "python 001_split_files.py\n",
    "\n",
    "\n",
    "[install]\n",
    "python -m pip install transformers\n",
    "python -m pip install pyarrow\n",
    "python -m pip install pandas\n",
    "python -m pip install numpy\n",
    "python -m pip install tensorflow\n",
    "python -m pip install sentencepiece\n",
    "\n",
    "[source]\n",
    "# multilingual\n",
    "https://huggingface.co/lxyuan/distilbert-base-multilingual-cased-sentiments-student\n",
    "\n",
    "# french\n",
    "https://huggingface.co/cmarkea/distilcamembert-base-sentiment\n",
    "\n",
    "The dataset comprises 204,993 reviews for training and 4,999 reviews for the test from Amazon, and 235,516 and 4,729 critics from Allocine website. The dataset is labeled into five categories:\n",
    "\n",
    "\n",
    "1 étoile : représente une appréciation terrible,\n",
    "2 étoiles : mauvaise appréciation,\n",
    "3 étoiles : appréciation neutre,\n",
    "4 étoiles : bonne appréciation,\n",
    "5 étoiles : excellente appréciation.\n",
    "\n",
    "1 star: represents a terrible appreciation,\n",
    "2 stars: bad appreciation,\n",
    "3 stars: neutral appreciation,\n",
    "4 stars: good appreciation,\n",
    "5 stars: excellent appreciation.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# DATA\n",
    "import numpy as np  # Importing numpy library and aliasing it as np\n",
    "import pandas as pd  # Importing pandas library and aliasing it as pd\n",
    "\n",
    "##### VALUES\n",
    "CSV_SOURCE=\"data_source/sentiment_analysis_reviews_0.csv\"  # Assigning a file path to CSV_SOURCE variable\n",
    "\n",
    "# Reading the data from CSV_SOURCE file into a pandas DataFrame\n",
    "data = pd.read_csv(CSV_SOURCE)\n",
    "print(data)  # Printing the DataFrame to the console\n",
    "\n",
    "# Define the number of CSV files to split the data into\n",
    "k = 50\n",
    "# Define the size of each split\n",
    "size = 20\n",
    "\n",
    "# Loop to split the data into k files\n",
    "for i in range(k):\n",
    "    # Slicing the DataFrame to select rows for the current split\n",
    "    df = data[size*i:size*(i+1)]\n",
    "    # Writing the selected rows to a new CSV file with a unique name\n",
    "    df.to_csv(f'data_split/sentiment_analysis_reviews_sample_{i+1}.csv', index=False)\n",
    "    # Printing a message indicating that the file has been created\n",
    "    print (f'the file data_split/sentiment_analysis_reviews_sample_{i+1}.csv has been created')\n",
    "\n",
    "print('\\n--- DONE')  # Printing a message indicating that the splitting process is done\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Engineering and Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the modeling phase, you often have to select the best model and iterate to see if the prediction is good. In the case of sentiment analysis, this is a known problem, the only difficulty is finding a model that has been trained on the language, in this case French. Everything is explained in the file `002_sentiment_analysis.py`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAUTION you must ensure that the source .csv files exist in the correct directory and that the destination directory exists, that is to say that the architecture of the project and the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
