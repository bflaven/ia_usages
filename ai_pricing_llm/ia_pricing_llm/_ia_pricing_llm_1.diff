

Pour 1 token en input, ajout automatioque de 7 tokens

500 tokens + 7 = 507




+ ARTICLE :: Reduce Your OpenAI API Costs by 70%
https://levelup.gitconnected.com/reduce-your-openai-api-costs-by-70-a9f123ce55a6

After all, we are nop communist and indeed in spite of the virtuus storytelling, IA is here to make money and to make a lot of 
blood cost a lot


! EXTRA INFORMATION
Pour info et pour garder une trace voilà les tarifs des API ChatGPT et Mistral noatmment input et output et sleon les modeles

1. Les tarifs précis de l'API ChatGPT
https://openai.com/api/pricing

2. Les tarifs précis de l'API Mistral voir section Pay as you Go
https://mistral.ai/fr/technology/#models


+ PART_1 Pricing exploration OpenAI and Mistral

--- prompt_tokens
the variable "prompt_tokens" refers to the input tokens, is 8. This is because every time you send text to the API, an additional 7 tokens are automatically added.

--- completion_tokens 
The value is what we expected, representing the tokens in the response generated by the model.

- See 001_reduce_api_costs.py, 002_reduce_api_costs.py, 003_reduce_api_costs.py

+ PART_2 Clustering using OpenAI API

Imagine you have a huge list of news headlines and you want to cluster them. While using embeddings is one option, let’s say you want to use OpenAI language models API, as it can capture meaning in a human-like way. Before we create the prompt template to minimize costs, let’s take a look at what our news headlines list looks like.


I’ve represented the news headlines in a shorter form, like s0, s1, s2, and so on. The reason for this is that when the language model clusters them, it can simply use these short abbreviations (e.g., s35 for the 35th news headline) instead of writing out the entire headline in each cluster.

--- see prompt_template in 003_reduce_api_costs.py

Next, I defined my prompt template. This template specifies the format of the answer I want the language model to provide, along with some additional information for clarity. The key here is that we’re not asking the model to write out the full headlines, but rather to just use the short abbreviations.

All we need to do is pass this prompt template to the function we created earlier and see how it performs in terms of pricing and response quality.

--- see prompt_template in 003_reduce_api_costs.py


+ PART_3 SpellCheck using OpenAI API

Let’s say you have a lengthy text document and you want to build a grammar correction tool as a small web app. While there are many NLP techniques available for this task, language models, particularly those from OpenAI, have been trained on vast amounts of data, making them a potentially better choice. Again, the key is to be strategic with our prompt template. We want the API response to highlight incorrect words and suggest their correct spellings, rather than providing the entire corrected text as output.


--- see prompt_template in 005_reduce_api_costs.py


+ PART_4 Text Cleaning using OpenAI API



The prompt template we used for spellcheck same will be used for text cleaning as we only need to highlight those words that either want to get cleaned or removed completely.

We want the API response to highlight incorrect words and suggest their correct spellings, rather than providing the entire corrected text as output. 

--- see prompt_template in 006_reduce_api_costs.py


+ PART_5 LLM Based NLP
I’ve developed an NLP library that uses LLM APIs to perform various tasks. It includes over 30 features, many of which works with similar cost-optimization strategies as described above. You can explore the library and its prompt templates in my GitHub repository: LLM Based NLP Library. basiclingua-LLM-Based-NLP


- BasicLINGUA
https://github.com/FareedKhan-dev/basiclingua-LLM-Based-NLP

--- Tasks resolved by BasicLINGUA
Entity Extraction, Text Summarization, Text Classification	, Text Sentiment Analysis, Text Coreference Resolution, Text Intent Recognition, Text OCR, Text Anomaly Detection, Text Sense Disambiguation, Text Spellcheck


! TODO Long document content extraction, check this ressource to undestand the input "100K_words_document_and_one_question" pass to the function.

# asking a question regarding 100k words document
openai_chat(100K_words_document_and_one_question)

https://cookbook.openai.com/examples/entity_extraction_for_long_documents


! SOURCE




--- Using Nvidia
Register and find out what can be used



