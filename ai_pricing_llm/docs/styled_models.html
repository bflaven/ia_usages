<style type="text/css">
#T_bc478 th {
  max-width: 85px;
  word-break: break-all;
}
#T_bc478_row0_col0, #T_bc478_row0_col2, #T_bc478_row0_col3, #T_bc478_row0_col4, #T_bc478_row1_col0, #T_bc478_row1_col2, #T_bc478_row1_col3, #T_bc478_row1_col4, #T_bc478_row2_col0, #T_bc478_row2_col2, #T_bc478_row2_col3, #T_bc478_row2_col4, #T_bc478_row3_col0, #T_bc478_row3_col2, #T_bc478_row3_col3, #T_bc478_row3_col4, #T_bc478_row4_col0, #T_bc478_row4_col2, #T_bc478_row4_col3, #T_bc478_row4_col4, #T_bc478_row5_col0, #T_bc478_row5_col2, #T_bc478_row5_col3, #T_bc478_row5_col4, #T_bc478_row6_col0, #T_bc478_row6_col2, #T_bc478_row6_col3, #T_bc478_row6_col4, #T_bc478_row7_col0, #T_bc478_row7_col2, #T_bc478_row7_col3, #T_bc478_row7_col4, #T_bc478_row8_col0, #T_bc478_row8_col2, #T_bc478_row8_col3, #T_bc478_row8_col4, #T_bc478_row9_col0, #T_bc478_row9_col2, #T_bc478_row9_col3, #T_bc478_row9_col4, #T_bc478_row10_col0, #T_bc478_row10_col2, #T_bc478_row10_col3, #T_bc478_row10_col4, #T_bc478_row11_col0, #T_bc478_row11_col2, #T_bc478_row11_col3, #T_bc478_row11_col4, #T_bc478_row12_col0, #T_bc478_row12_col2, #T_bc478_row12_col3, #T_bc478_row12_col4, #T_bc478_row13_col0, #T_bc478_row13_col2, #T_bc478_row13_col3, #T_bc478_row13_col4, #T_bc478_row14_col0, #T_bc478_row14_col2, #T_bc478_row14_col3, #T_bc478_row14_col4, #T_bc478_row15_col0, #T_bc478_row15_col2, #T_bc478_row15_col3, #T_bc478_row15_col4, #T_bc478_row16_col0, #T_bc478_row16_col2, #T_bc478_row16_col3, #T_bc478_row16_col4, #T_bc478_row17_col0, #T_bc478_row17_col2, #T_bc478_row17_col3, #T_bc478_row17_col4, #T_bc478_row18_col0, #T_bc478_row18_col2, #T_bc478_row18_col3, #T_bc478_row18_col4, #T_bc478_row19_col0, #T_bc478_row19_col2, #T_bc478_row19_col3, #T_bc478_row19_col4, #T_bc478_row20_col0, #T_bc478_row20_col2, #T_bc478_row20_col3, #T_bc478_row20_col4, #T_bc478_row21_col0, #T_bc478_row21_col2, #T_bc478_row21_col3, #T_bc478_row21_col4, #T_bc478_row22_col0, #T_bc478_row22_col2, #T_bc478_row22_col3, #T_bc478_row22_col4, #T_bc478_row23_col0, #T_bc478_row23_col2, #T_bc478_row23_col3, #T_bc478_row23_col4, #T_bc478_row24_col0, #T_bc478_row24_col2, #T_bc478_row24_col3, #T_bc478_row24_col4, #T_bc478_row25_col0, #T_bc478_row25_col2, #T_bc478_row25_col3, #T_bc478_row25_col4 {
  max-width: 80px;
}
#T_bc478_row0_col1, #T_bc478_row1_col1, #T_bc478_row2_col1, #T_bc478_row3_col1, #T_bc478_row4_col1, #T_bc478_row5_col1, #T_bc478_row6_col1, #T_bc478_row7_col1, #T_bc478_row8_col1, #T_bc478_row9_col1, #T_bc478_row10_col1, #T_bc478_row11_col1, #T_bc478_row12_col1, #T_bc478_row13_col1, #T_bc478_row14_col1, #T_bc478_row15_col1, #T_bc478_row16_col1, #T_bc478_row17_col1, #T_bc478_row18_col1, #T_bc478_row19_col1, #T_bc478_row20_col1, #T_bc478_row21_col1, #T_bc478_row22_col1, #T_bc478_row23_col1, #T_bc478_row24_col1, #T_bc478_row25_col1 {
  max-width: 80px;
  max-width: 280px;
}
</style>
<table id="T_bc478">
  <thead>
    <tr>
      <th id="T_bc478_level0_col0" class="col_heading level0 col0" >name</th>
      <th id="T_bc478_level0_col1" class="col_heading level0 col1" >description</th>
      <th id="T_bc478_level0_col2" class="col_heading level0 col2" >max_tokens</th>
      <th id="T_bc478_level0_col3" class="col_heading level0 col3" >prompt_cost_per_token</th>
      <th id="T_bc478_level0_col4" class="col_heading level0 col4" >completion_cost_per_token</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td id="T_bc478_row0_col0" class="data row0 col0" >gpt-3.5-turbo-instruct</td>
      <td id="T_bc478_row0_col1" class="data row0 col1" >Same capabilities as the standard gpt-3.5-turbo model but uses completion rather than chat completion enpoint</td>
      <td id="T_bc478_row0_col2" class="data row0 col2" >4096</td>
      <td id="T_bc478_row0_col3" class="data row0 col3" >0.0015 / 1000</td>
      <td id="T_bc478_row0_col4" class="data row0 col4" >0.002 / 1000</td>
    </tr>
    <tr>
      <td id="T_bc478_row1_col0" class="data row1 col0" >gpt-3.5-turbo</td>
      <td id="T_bc478_row1_col1" class="data row1 col1" >Most capable GPT-3.5 model and optimized for chat at 1/10th the cost of text-davinci-003. Will be updated with our latest model iteration.</td>
      <td id="T_bc478_row1_col2" class="data row1 col2" >4096</td>
      <td id="T_bc478_row1_col3" class="data row1 col3" >0.0015 / 1000</td>
      <td id="T_bc478_row1_col4" class="data row1 col4" >0.002 / 1000</td>
    </tr>
    <tr>
      <td id="T_bc478_row2_col0" class="data row2 col0" >gpt-3.5-turbo-16k</td>
      <td id="T_bc478_row2_col1" class="data row2 col1" >Same capabilities as the standard gpt-3.5-turbo model but with 4 times the context.</td>
      <td id="T_bc478_row2_col2" class="data row2 col2" >16385</td>
      <td id="T_bc478_row2_col3" class="data row2 col3" >0.003 / 1000</td>
      <td id="T_bc478_row2_col4" class="data row2 col4" >0.004 / 1000</td>
    </tr>
    <tr>
      <td id="T_bc478_row3_col0" class="data row3 col0" >gpt-4-32k</td>
      <td id="T_bc478_row3_col1" class="data row3 col1" >Same capabilities as the base gpt-4 mode but with 4x the context length. Will be updated with our latest model iteration.</td>
      <td id="T_bc478_row3_col2" class="data row3 col2" >32768</td>
      <td id="T_bc478_row3_col3" class="data row3 col3" >0.06 / 1000</td>
      <td id="T_bc478_row3_col4" class="data row3 col4" >0.12 / 1000</td>
    </tr>
    <tr>
      <td id="T_bc478_row4_col0" class="data row4 col0" >gpt-4</td>
      <td id="T_bc478_row4_col1" class="data row4 col1" >More capable than any GPT-3.5 model, able to do more complex tasks, and optimized for chat. Will be updated with our latest model iteration.</td>
      <td id="T_bc478_row4_col2" class="data row4 col2" >8192</td>
      <td id="T_bc478_row4_col3" class="data row4 col3" >0.03 / 1000</td>
      <td id="T_bc478_row4_col4" class="data row4 col4" >0.06 / 1000</td>
    </tr>
    <tr>
      <td id="T_bc478_row5_col0" class="data row5 col0" >text-ada-001</td>
      <td id="T_bc478_row5_col1" class="data row5 col1" >Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.</td>
      <td id="T_bc478_row5_col2" class="data row5 col2" >2049</td>
      <td id="T_bc478_row5_col3" class="data row5 col3" >0.0004 / 1000</td>
      <td id="T_bc478_row5_col4" class="data row5 col4" >0.0004 / 1000</td>
    </tr>
    <tr>
      <td id="T_bc478_row6_col0" class="data row6 col0" >text-babbage-001</td>
      <td id="T_bc478_row6_col1" class="data row6 col1" >Capable of straightforward tasks, very fast, and lower cost.</td>
      <td id="T_bc478_row6_col2" class="data row6 col2" >2049</td>
      <td id="T_bc478_row6_col3" class="data row6 col3" >0.0005 / 1000</td>
      <td id="T_bc478_row6_col4" class="data row6 col4" >0.0005 / 1000</td>
    </tr>
    <tr>
      <td id="T_bc478_row7_col0" class="data row7 col0" >text-curie-001</td>
      <td id="T_bc478_row7_col1" class="data row7 col1" >Very capable, faster and lower cost than Davinci.</td>
      <td id="T_bc478_row7_col2" class="data row7 col2" >2049</td>
      <td id="T_bc478_row7_col3" class="data row7 col3" >0.002 / 1000</td>
      <td id="T_bc478_row7_col4" class="data row7 col4" >0.002 / 1000</td>
    </tr>
    <tr>
      <td id="T_bc478_row8_col0" class="data row8 col0" >text-davinci-001</td>
      <td id="T_bc478_row8_col1" class="data row8 col1" >None</td>
      <td id="T_bc478_row8_col2" class="data row8 col2" >8001</td>
      <td id="T_bc478_row8_col3" class="data row8 col3" >0.02 / 1000</td>
      <td id="T_bc478_row8_col4" class="data row8 col4" >0.02 / 1000</td>
    </tr>
    <tr>
      <td id="T_bc478_row9_col0" class="data row9 col0" >text-davinci-002</td>
      <td id="T_bc478_row9_col1" class="data row9 col1" >Similar capabilities to text-davinci-003 but trained with supervised fine-tuning instead of reinforcement learning</td>
      <td id="T_bc478_row9_col2" class="data row9 col2" >4097</td>
      <td id="T_bc478_row9_col3" class="data row9 col3" >0.02 / 1000</td>
      <td id="T_bc478_row9_col4" class="data row9 col4" >0.02 / 1000</td>
    </tr>
    <tr>
      <td id="T_bc478_row10_col0" class="data row10 col0" >text-davinci-003</td>
      <td id="T_bc478_row10_col1" class="data row10 col1" >Most capable GPT-3 model. Can do any task the other models can do, often with higher quality.</td>
      <td id="T_bc478_row10_col2" class="data row10 col2" >4097</td>
      <td id="T_bc478_row10_col3" class="data row10 col3" >0.02 / 1000</td>
      <td id="T_bc478_row10_col4" class="data row10 col4" >0.02 / 1000</td>
    </tr>
    <tr>
      <td id="T_bc478_row11_col0" class="data row11 col0" >gpt-4-0314</td>
      <td id="T_bc478_row11_col1" class="data row11 col1" >Snapshot of gpt-4 from March 14th 2023. Unlike gpt-4, this model will not receive updates, and will be deprecated 3 months after a new version is released.</td>
      <td id="T_bc478_row11_col2" class="data row11 col2" >8192</td>
      <td id="T_bc478_row11_col3" class="data row11 col3" >0.03 / 1000</td>
      <td id="T_bc478_row11_col4" class="data row11 col4" >0.06 / 1000</td>
    </tr>
    <tr>
      <td id="T_bc478_row12_col0" class="data row12 col0" >gpt-4-0613</td>
      <td id="T_bc478_row12_col1" class="data row12 col1" >Snapshot of gpt-4 from June 13th 2023 with function calling data. Unlike gpt-4, this model will not receive updates, and will be deprecated 3 months after a new version is released.</td>
      <td id="T_bc478_row12_col2" class="data row12 col2" >8192</td>
      <td id="T_bc478_row12_col3" class="data row12 col3" >0.03 / 1000</td>
      <td id="T_bc478_row12_col4" class="data row12 col4" >0.06 / 1000</td>
    </tr>
    <tr>
      <td id="T_bc478_row13_col0" class="data row13 col0" >gpt-4-32k-0314</td>
      <td id="T_bc478_row13_col1" class="data row13 col1" >Snapshot of gpt-4-32 from March 14th 2023. Unlike gpt-4-32k, this model will not receive updates, and will be deprecated 3 months after a new version is released.</td>
      <td id="T_bc478_row13_col2" class="data row13 col2" >32768</td>
      <td id="T_bc478_row13_col3" class="data row13 col3" >0.06 / 1000</td>
      <td id="T_bc478_row13_col4" class="data row13 col4" >0.12 / 1000</td>
    </tr>
    <tr>
      <td id="T_bc478_row14_col0" class="data row14 col0" >gpt-4-32k-0613</td>
      <td id="T_bc478_row14_col1" class="data row14 col1" >Snapshot of gpt-4-32 from June 13th 2023. Unlike gpt-4-32k, this model will not receive updates, and will be deprecated 3 months after a new version is released.</td>
      <td id="T_bc478_row14_col2" class="data row14 col2" >32768</td>
      <td id="T_bc478_row14_col3" class="data row14 col3" >0.06 / 1000</td>
      <td id="T_bc478_row14_col4" class="data row14 col4" >0.12 / 1000</td>
    </tr>
    <tr>
      <td id="T_bc478_row15_col0" class="data row15 col0" >gpt-3.5-turbo-0301</td>
      <td id="T_bc478_row15_col1" class="data row15 col1" >Snapshot of gpt-3.5-turbo from March 1st 2023. Unlike gpt-3.5-turbo, this model will not receive updates, and will be deprecated 3 months after a new version is released.</td>
      <td id="T_bc478_row15_col2" class="data row15 col2" >4096</td>
      <td id="T_bc478_row15_col3" class="data row15 col3" >0.002 / 1000</td>
      <td id="T_bc478_row15_col4" class="data row15 col4" >0.002 / 1000</td>
    </tr>
    <tr>
      <td id="T_bc478_row16_col0" class="data row16 col0" >gpt-3.5-turbo-0613</td>
      <td id="T_bc478_row16_col1" class="data row16 col1" >Snapshot of gpt-3.5-turbo from June 13th 2023 with function calling data. Unlike gpt-3.5-turbo, this model will not receive updates, and will be deprecated 3 months after a new version is released.</td>
      <td id="T_bc478_row16_col2" class="data row16 col2" >4096</td>
      <td id="T_bc478_row16_col3" class="data row16 col3" >0.0015 / 1000</td>
      <td id="T_bc478_row16_col4" class="data row16 col4" >0.002 / 1000</td>
    </tr>
    <tr>
      <td id="T_bc478_row17_col0" class="data row17 col0" >gpt-3.5-turbo-16k-0613</td>
      <td id="T_bc478_row17_col1" class="data row17 col1" >Snapshot of gpt-3.5-turbo-16k from June 13th 2023. Unlike gpt-3.5-turbo-16k, this model will not receive updates, and will be deprecated 3 months after a new version is released.</td>
      <td id="T_bc478_row17_col2" class="data row17 col2" >16385</td>
      <td id="T_bc478_row17_col3" class="data row17 col3" >0.003 / 1000</td>
      <td id="T_bc478_row17_col4" class="data row17 col4" >0.004 / 1000</td>
    </tr>
    <tr>
      <td id="T_bc478_row18_col0" class="data row18 col0" >code-davinci-002</td>
      <td id="T_bc478_row18_col1" class="data row18 col1" >Optimized for code-completion tasks</td>
      <td id="T_bc478_row18_col2" class="data row18 col2" >8001</td>
      <td id="T_bc478_row18_col3" class="data row18 col3" >0.002 / 1000</td>
      <td id="T_bc478_row18_col4" class="data row18 col4" >0.002 / 1000</td>
    </tr>
    <tr>
      <td id="T_bc478_row19_col0" class="data row19 col0" >davinci</td>
      <td id="T_bc478_row19_col1" class="data row19 col1" >Most capable GPT-3 model. Can do any task the other models can do, often with higher quality.</td>
      <td id="T_bc478_row19_col2" class="data row19 col2" >2049</td>
      <td id="T_bc478_row19_col3" class="data row19 col3" >0.002 / 1000</td>
      <td id="T_bc478_row19_col4" class="data row19 col4" >0.0200 / 1000</td>
    </tr>
    <tr>
      <td id="T_bc478_row20_col0" class="data row20 col0" >curie</td>
      <td id="T_bc478_row20_col1" class="data row20 col1" >Very capable, but faster and lower cost than Davinci.</td>
      <td id="T_bc478_row20_col2" class="data row20 col2" >2049</td>
      <td id="T_bc478_row20_col3" class="data row20 col3" >None</td>
      <td id="T_bc478_row20_col4" class="data row20 col4" >0.0020 / 1000</td>
    </tr>
    <tr>
      <td id="T_bc478_row21_col0" class="data row21 col0" >babbage</td>
      <td id="T_bc478_row21_col1" class="data row21 col1" >Capable of straightforward tasks, very fast, and lower cost.</td>
      <td id="T_bc478_row21_col2" class="data row21 col2" >2049</td>
      <td id="T_bc478_row21_col3" class="data row21 col3" >None</td>
      <td id="T_bc478_row21_col4" class="data row21 col4" >0.0005 / 1000</td>
    </tr>
    <tr>
      <td id="T_bc478_row22_col0" class="data row22 col0" >ada</td>
      <td id="T_bc478_row22_col1" class="data row22 col1" >Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.</td>
      <td id="T_bc478_row22_col2" class="data row22 col2" >2049</td>
      <td id="T_bc478_row22_col3" class="data row22 col3" >None</td>
      <td id="T_bc478_row22_col4" class="data row22 col4" >0.0004 / 1000</td>
    </tr>
    <tr>
      <td id="T_bc478_row23_col0" class="data row23 col0" >code-davinci-001</td>
      <td id="T_bc478_row23_col1" class="data row23 col1" >Earlier version of code-davinci-002</td>
      <td id="T_bc478_row23_col2" class="data row23 col2" >8001</td>
      <td id="T_bc478_row23_col3" class="data row23 col3" >0.002 / 1000</td>
      <td id="T_bc478_row23_col4" class="data row23 col4" >0.002 / 1000</td>
    </tr>
    <tr>
      <td id="T_bc478_row24_col0" class="data row24 col0" >code-cushman-002</td>
      <td id="T_bc478_row24_col1" class="data row24 col1" >Almost as capable as Davinci Codex, but slightly faster. This speed advantage may make it preferable for real-time applications.</td>
      <td id="T_bc478_row24_col2" class="data row24 col2" >2048</td>
      <td id="T_bc478_row24_col3" class="data row24 col3" >None</td>
      <td id="T_bc478_row24_col4" class="data row24 col4" >None</td>
    </tr>
    <tr>
      <td id="T_bc478_row25_col0" class="data row25 col0" >code-cushman-001</td>
      <td id="T_bc478_row25_col1" class="data row25 col1" >Earlier version of code-cushman-002</td>
      <td id="T_bc478_row25_col2" class="data row25 col2" >2048</td>
      <td id="T_bc478_row25_col3" class="data row25 col3" >None</td>
      <td id="T_bc478_row25_col4" class="data row25 col4" >None</td>
    </tr>
  </tbody>
</table>
