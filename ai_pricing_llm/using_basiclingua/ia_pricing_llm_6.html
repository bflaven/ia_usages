

<!DOCTYPE html>
<html>
<body>

<title>template-blog-article</title>


<!-- TITLE: -->

<!-- KWS: -->

After all, "we are not communist" says Emilio "The Wolf" Barzini in the Godafather* and indeed in spite of the virtuous storytelling, IA is here to make money and to make a lot... 

* I watched Coppola's The Godfather once again which is, in addition to being a great film about the Italian-American mafia, a great lesson on capitalism and team management!

More seriously, as the PO of an AI-based product, the question of price quickly arises. I'm not talking about any development costs but much more the price of an API key and its use via prompts. Indeed, IA companies are not philanthropists, and their economic models are based on addiction. Indeed, the pay-as-you-go pricing system is exceptionally "poisonous" as the more you outsource to IA, the more you pay.

For the last week, I have decided to go with ChatGPT and Mistral API key, so I was force to scrutinize the pricing pages for input and output according to the models. Here is the ressources: 


<ol>
<lI>The precise prices of the ChatGPT API: <a href="https://openai.com/api/pricing" target="_blank" rel="noopener">https://openai.com/api/pricing</a>

</li>

<li>The precise prices of the Mistral API see the Pay as you Go section: <a href="https://mistral.ai/fr/technology/#models" target="_blank" rel="noopener">https://mistral.ai/fr/technology/#models</a></li>

</ol>



I made a search on explanations about the ChatGPT and Mistral pricing because I am lazy. I found good ressources, especially this post: "Reduce Your OpenAI API Costs by 70%"" at <a href="https://levelup.gitconnected.com/reduce-your-openai-api-costs-by-70-a9f123ce55a6" target="_blank" rel="noopener">https://levelup.gitconnected.com/reduce-your-openai-api-costs-by-70-a9f123ce55a6</a>


<b>This post introduces a fruitful correlation between cost efficiency and prompt design patterns!</b>


<b>Here are notions quickly summarized that I kept from this post. The very first thing that you need to know is that, when you connect with an LLM via an API key, you are billed when sending content to the LLM (input) and when receiving content from the LLM (output).</b>

<i>1. Number of tokens for a prompt</i>
The guy discover that they are additionnal

As an example, for ChatGPT, we can play with this tool which calculates the tokens number from a text: <a href="https://www.tokencounter.io/" target="_blank" rel="noopener">https://www.tokencounter.io/</a>


<b>For LLM, the key element the Token so what you need to know is "what a token is?" which is the basic element that will be "input" and "output" by the LLM.</b>

<b>For example, for an English text, 1 token corresponds to approximately 4 characters or 0.75 words. For reference, Shakespeare's collected works are approximately 900,000 words or 1.2 million tokens.</b>


<pre>
# some examples

1 token ~= 4 chars in English
1 token ~= ¾ words
100 tokens ~= 75 words

1-2 sentence(s) ~= 30 tokens
1 paragraph ~= 100 tokens
1,500 words ~= 2048 tokens
2,000 words ~= 2730 tokens
</pre>


<blockquote>The variable "prompt_tokens" refers to the input tokens, is 8. This is because every time you send text to the API, an additional 7 tokens are automatically added.</blockquote> 

<i>2. Number of tokens for completion</i> 
You will pay also what is produced by the LLM in the form of tokens.

<blockquote> The value is what we expected, representing the tokens in the response generated by the model.</blockquote> 

<b>See 001_reduce_api_costs.py, 002_reduce_api_costs.py, 003_reduce_api_costs.py</b>

<i>3. Clustering using OpenAI API</i>

<b>Clustering, a great idea that must be explore if you intend to leverage on LLM API KEY.</b>

Here a quick definition of the user need.

<blockquote> Imagine you have a huge list of news headlines and you want to cluster them. While using embeddings is one option, let’s say you want to use OpenAI language models API, as it can capture meaning in a human-like way. Before we create the prompt template to minimize costs, let’s take a look at what our news headlines list looks like. I've represented the news headlines in a shorter form, like s0, s1, s2, and so on. The reason for this is that when the language model clusters them, it can simply use these short abbreviations (e.g., s35 for the 35th news headline) instead of writing out the entire headline in each cluster.</blockquote>

Here is also a simple explanation of the expectation.


<blockquote> Next, I defined my prompt template. This template specifies the format of the answer I want the language model to provide, along with some additional information for clarity. The key here is that we’re not asking the model to write out the full headlines, but rather to just use the short abbreviations. All we need to do is pass this prompt template to the function we created earlier and see how it performs in terms of pricing and response quality.</blockquote> 


<b>See prompt_template in 003_reduce_api_costs.py</b>


<i>4. SpellCheck using OpenAI API</i>

Let’s say you have a lengthy text document and you want to build a grammar correction tool as a small web app. While there are many NLP techniques available for this task, language models, particularly those from OpenAI, have been trained on vast amounts of data, making them a potentially better choice. Again, the key is to be strategic with our prompt template. We want the API response to highlight incorrect words and suggest their correct spellings, rather than providing the entire corrected text as output.


<b>See prompt_template in 005_reduce_api_costs.py</b>

<i>5. Text Cleaning using OpenAI API</i>

The prompt template we used for spellcheck same will be used for text cleaning as we only need to highlight those words that either want to get cleaned or removed completely.

We want the API response to highlight incorrect words and suggest their correct spellings, rather than providing the entire corrected text as output. 

<b>See prompt_template in 006_reduce_api_costs.py</b>

<h2>Exploring the pricing of the NVIDIA platform</h2>
There is also a fairly simple way to explore different types of LLMs, a bit like huggingface.co, which is to rely on the NVIDIA platform which provides a turnkey package of LLMs and a handful of credits for start your tests. Once you have overcome the complexity of the site, you have access to code and a set of models: llama3-70b, phi-3-mini, codegemma-7b, mistral-large…

<code>
https://build.nvidia.com/explore/discover#llama3-70b
https://build.nvidia.com/explore/discover#phi-3-mini
https://build.nvidia.com/explore/discover#codegemma-7b
https://build.nvidia.com/mistralai/mistral-large
... etc
</code>

<h2>Exploring basiclingua-LLM-Based-NLP</h2>
The author of the post "Reduce Your OpenAI API Costs by 70%" have developed an NLP library that uses LLM APIs to perform various tasks. It includes over 30 features, many of which works with similar cost-optimization strategies as described in this post.  

<b>The code available on the GitHub repository possess numerous prompts resources that can provide great sample and models to learn prompting.</b>

<b>Tasks resolved by BasicLINGUA</b>
Entity Extraction, Text Summarization, Text Classification  , Text Sentiment Analysis, Text Coreference Resolution, Text Intent Recognition, Text OCR, Text Anomaly Detection, Text Sense Disambiguation, Text Spellcheck


<pre lang="python">



############## EXTRACT PATTERNS ##############

		# Generate the prompt template
        prompt_template = f'''
        Given the input text:
        user input: {user_input}

        extract following patterns from it: {patterns}

        output must be a python dictionary with keys as patterns and values as list of extracted patterns
        '''




############## NER EXTRACTION ##############

		# check if parameters are of correct type
        if not isinstance(user_input, str):
            raise TypeError("user_input must be of type str")
        if not isinstance(ner_tags, str):
            raise TypeError("ner_tags must be of type str")

        # check if parameters are not empty
        if not user_input:
            raise ValueError("user_input cannot be empty")

        # user ner tags
        if ner_tags != "":
            user_ner_tags = f'''NER TAGS: {ner_tags}'''
        else:
            user_ner_tags = f'''NER TAGS: FAC, CARDINAL, NUMBER, DEMONYM, QUANTITY, TITLE, PHONE_NUMBER, NATIONAL, JOB, PERSON, LOC, NORP, TIME, CITY, EMAIL, GPE, LANGUAGE, PRODUCT, ZIP_CODE, ADDRESS, MONEY, ORDINAL, DATE, EVENT, CRIMINAL_CHARGE, STATE_OR_PROVINCE, RELIGION, DURATION, URL, WORK_OF_ART, PERCENT, CAUSE_OF_DEATH, COUNTRY, ORG, LAW, NAME, COUNTRY, RELIGION, TIME'''

        # Generate the prompt template
        prompt_template = f'''Given the input text:
        user input: {user_input}
        
        perform NER detection on it.
        {user_ner_tags}
        answer must be in the format
        tag:value
        '''

</pre>



Source: <a href="https://github.com/FareedKhan-dev/basiclingua-LLM-Based-NLP" target="_blank" rel="noopener">https://github.com/FareedKhan-dev/basiclingua-LLM-Based-NLP</a>


<b>Simple Token definition</b>
<blockquote>You can think of tokens as pieces of words used for natural language processing. For English text, 1 token is approximately 4 characters or 0.75 words. As a point of reference, the collected works of Shakespeare are about 900,000 words or 1.2M tokens.</blockquote>

Source: <a href="https://platform.openai.com/tokenizer" target="_blank" rel="noopener">https://platform.openai.com/tokenizer</a>


<h2>A quick schematic process of validation of each AI feature</h2>
As a reminder, here is a detailed process to validate an IA feature. The validation process will be the same for each use case.

<ul>
<li>Phase_1: R&D phase with POCs</li>

<li>Phase_2: User feedback to gauge quality, artisanal validation, progress on the quality level. </li>


<li>Phase_3: Pilot phase integration into a business tool such as Backoffice. This pilot phase makes it possible to extend the validation phase to a larger sample of content and uses. This integration will be done via Feature flipping (the AI feature is only available to a limited number of users). This makes it possible to test/validate by having usage feedback on possible problems (performance, connectivity, refine usage based on production content, etc.). This is a Fine-tuning phase.</li>

<li>Phase_4: Put into production, opening of the feature to all users in the business tool e.g. Backoffice. The feature nevertheless continues to evolve as a result of improvement of the existing model or by changing the model.</li>
</ul>


As soon as the integration is done in a business tool, phase_3 must be carried out identically.


<H2>More infos</H2>


<ul>


<li>Reduce Your OpenAI API Costs by 70% | by Fareed Khan | Mar, 2024 | Level Up Coding<br><a href="https://levelup.gitconnected.com/reduce-your-openai-api-costs-by-70-a9f123ce55a6" target="_blank" rel="noopener">https://levelup.gitconnected.com/reduce-your-openai-api-costs-by-70-a9f123ce55a6</a></li><li>Title Not Found<br><a href="https://openai.com/api/pricing" target="_blank" rel="noopener">https://openai.com/api/pricing</a></li><li>Technologie | Mistral AI | Frontier AI in your hands<br><a href="https://mistral.ai/fr/technology/#models" target="_blank" rel="noopener">https://mistral.ai/fr/technology/#models</a></li><li>GitHub - FareedKhan-dev/basiclingua-LLM-Based-NLP: LLM Based NLP Library.<br><a href="https://github.com/FareedKhan-dev/basiclingua-LLM-Based-NLP" target="_blank" rel="noopener">https://github.com/FareedKhan-dev/basiclingua-LLM-Based-NLP</a></li><li>Long document content extraction | OpenAI Cookbook<br><a href="https://cookbook.openai.com/examples/entity_extraction_for_long_documents" target="_blank" rel="noopener">https://cookbook.openai.com/examples/entity_extraction_for_long_documents</a></li><li>Reduce Your OpenAI API Costs by 70% | by Fareed Khan | Mar, 2024 | Level Up Coding<br><a href="https://levelup.gitconnected.com/reduce-your-openai-api-costs-by-70-a9f123ce55a6" target="_blank" rel="noopener">https://levelup.gitconnected.com/reduce-your-openai-api-costs-by-70-a9f123ce55a6</a></li><li>GitHub - FareedKhan-dev/basiclingua-LLM-Based-NLP: LLM Based NLP Library.<br><a href="https://github.com/FareedKhan-dev/basiclingua-LLM-Based-NLP" target="_blank" rel="noopener">https://github.com/FareedKhan-dev/basiclingua-LLM-Based-NLP</a></li><li>Understanding OpenAI API Cost In-Depth Using a Real Example - WordBot<br><a href="https://blog.wordbot.io/ai-artificial-intelligence/understanding-gpt3-cost-in-depth-using-a-real-example/" target="_blank" rel="noopener">https://blog.wordbot.io/ai-artificial-intelligence/understanding-gpt3-cost-in-depth-using-a-real-example/</a></li><li>TokenCounter: tokenize and estimate your LLM costs<br><a href="https://www.tokencounter.io/" target="_blank" rel="noopener">https://www.tokencounter.io/</a></li><li>Create Your Azure Free Account Today | Microsoft Azure<br><a href="https://azure.microsoft.com/en-us/free/ai-services/" target="_blank" rel="noopener">https://azure.microsoft.com/en-us/free/ai-services/</a></li><li>
Mistral Large now available on Azure 
<br><a href="https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/mistral-large-mistral-ai-s-flagship-llm-debuts-on-azure-ai/ba-p/4066996" target="_blank" rel="noopener">https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/mistral-large-mistral-ai-s-flagship-llm-debuts-on-azure-ai/ba-p/4066996</a></li><li>GitHub - openai/tiktoken: tiktoken is a fast BPE tokeniser for use with OpenAI's models.<br><a href="https://github.com/openai/tiktoken" target="_blank" rel="noopener">https://github.com/openai/tiktoken</a></li><li>GitHub - Promptly-Technologies-LLC/llm_cost_estimation: A simple Python library for estimating what the cost of an API call will be<br><a href="https://github.com/Promptly-Technologies-LLC/llm_cost_estimation" target="_blank" rel="noopener">https://github.com/Promptly-Technologies-LLC/llm_cost_estimation</a></li><li>GitHub - microsoft/LLMLingua: To speed up LLMs' inference and enhance LLM's perceive of key information, compress the prompt and KV-Cache, which achieves up to 20x compression with minimal performance loss.<br><a href="https://github.com/microsoft/LLMLingua" target="_blank" rel="noopener">https://github.com/microsoft/LLMLingua</a></li><li>GitHub - magdalenakuhn17/awesome-cheap-llms: Cost reduction tools and techniques for LLM based systems<br><a href="https://github.com/magdalenakuhn17/awesome-cheap-llms" target="_blank" rel="noopener">https://github.com/magdalenakuhn17/awesome-cheap-llms</a></li><li>GitHub - AnthusAI/LLM-Price-Comparison: A comparison of the price per million tokens and benchmark scores of various large language models.<br><a href="https://github.com/AnthusAI/LLM-Price-Comparison" target="_blank" rel="noopener">https://github.com/AnthusAI/LLM-Price-Comparison</a></li><li>Build a Token Counter and Cost Estimator with Streamlit and OpenAI | by Tony Esposito | Medium<br><a href="https://medium.com/@fbanespo/build-a-token-counter-and-cost-estimator-with-streamlit-and-openai-2181e603f7cb" target="_blank" rel="noopener">https://medium.com/@fbanespo/build-a-token-counter-and-cost-estimator-with-streamlit-and-openai-2181e603f7cb</a></li><li>The Ultimate Pricing Cheat-Sheet for Large Language Models<br><a href="https://www.newtuple.com/post/the-ultimate-pricing-cheat-sheet-for-large-language-models" target="_blank" rel="noopener">https://www.newtuple.com/post/the-ultimate-pricing-cheat-sheet-for-large-language-models</a></li><li>Cost Analysis of deploying LLMs: A comparative Study between Cloud Managed, Self-Hosted and 3rd Party LLMs | by Hugo Debes | Artefact Engineering and Data Science | Medium<br><a href="https://medium.com/artefact-engineering-and-data-science/llms-deployment-a-practical-cost-analysis-e0c1b8eb08ca" target="_blank" rel="noopener">https://medium.com/artefact-engineering-and-data-science/llms-deployment-a-practical-cost-analysis-e0c1b8eb08ca</a></li><li>Your request has been blocked. This could be
due to several reasons.<br><a href="https://www.microsoft.com/en-us/research/group/experimentation-platform-exp/articles/how-to-evaluate-llms-a-complete-metric-framework/" target="_blank" rel="noopener">https://www.microsoft.com/en-us/research/group/experimentation-platform-exp/articles/how-to-evaluate-llms-a-complete-metric-framework/</a></li><li>llm-cost-estimation — llm-cost-estimator latest documentation<br><a href="https://llm-cost-estimator.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">https://llm-cost-estimator.readthedocs.io/en/latest/index.html</a></li><li>GitHub - egordm/RougLLy: Quick and Realistic Cost Estimation for LLMs.<br><a href="https://github.com/egordm/RougLLy" target="_blank" rel="noopener">https://github.com/egordm/RougLLy</a></li><li>Paul Simmering – LLM Price Comparison<br><a href="https://simmering.dev/blog/llm-price-performance/" target="_blank" rel="noopener">https://simmering.dev/blog/llm-price-performance/</a></li><li>Understanding the cost of Large Language Models (LLMs)<br><a href="https://www.tensorops.ai/post/understanding-the-cost-of-large-language-models-llms" target="_blank" rel="noopener">https://www.tensorops.ai/post/understanding-the-cost-of-large-language-models-llms</a></li><li>Advanced Prompt Engineering - Practical Examples<br><a href="https://www.tensorops.ai/post/prompt-engineering-techniques-practical-guide" target="_blank" rel="noopener">https://www.tensorops.ai/post/prompt-engineering-techniques-practical-guide</a></li><li>GitHub - TensorOpsAI/LLMstudio: Framework to bring LLM applications to production<br><a href="https://github.com/TensorOpsAI/LLMStudio" target="_blank" rel="noopener">https://github.com/TensorOpsAI/LLMStudio</a></li><li>TokenCounter: tokenize and estimate your LLM costs<br><a href="https://www.tokencounter.io/" target="_blank" rel="noopener">https://www.tokencounter.io/</a></li><li>LLM Price Calculator<br><a href="https://www.llmcalc.com/" target="_blank" rel="noopener">https://www.llmcalc.com/</a></li><li>Compare LLM API Pricing Instantly - Get the Best Deals at LLM Price Check<br><a href="https://llmpricecheck.com/" target="_blank" rel="noopener">https://llmpricecheck.com/</a></li><li>LLM Pricing Calculator - LLM Price Check<br><a href="https://llmpricecheck.com/calculator" target="_blank" rel="noopener">https://llmpricecheck.com/calculator</a></li><li>llm_cost_estimation · PyPI<br><a href="https://pypi.org/project/llm_cost_estimation/" target="_blank" rel="noopener">https://pypi.org/project/llm_cost_estimation/</a></li><li>LLM Pricing - Compare Large Language Model Costs and Pricing<br><a href="https://llm-price.com/" target="_blank" rel="noopener">https://llm-price.com/</a></li><li>GitHub - g-simmons/llm-cost-estimator: A simple cost estimator for batch text generation with OpenAI LLMs<br><a href="https://github.com/g-simmons/llm-cost-estimator" target="_blank" rel="noopener">https://github.com/g-simmons/llm-cost-estimator</a></li><li>GitHub - AgentOps-AI/tokencost: Easy token price estimates for LLMs<br><a href="https://github.com/AgentOps-AI/tokencost" target="_blank" rel="noopener">https://github.com/AgentOps-AI/tokencost</a></li><li>50+ Open-Source Options for Running LLMs Locally - Vince Lam<br><a href="https://vinlam.com/posts/local-llm-options/" target="_blank" rel="noopener">https://vinlam.com/posts/local-llm-options/</a></li><li>GitHub - VidhyaVarshanyJS/EnsembleX: EnsembleX utilizes the Knapsack algorithm to optimize Large Language Model (LLM) ensembles for quality-cost trade-offs, offering tailored suggestions across various domains through a Streamlit dashboard visualization.<br><a href="https://github.com/VidhyaVarshanyJS/EnsembleX" target="_blank" rel="noopener">https://github.com/VidhyaVarshanyJS/EnsembleX</a></li><li>Title Not Found<br><a href="https://platform.openai.com/tokenizer" target="_blank" rel="noopener">https://platform.openai.com/tokenizer</a></li><li>Log in with Atlassian account<br><a href="https://francemm.atlassian.net/browse/IA-98" target="_blank" rel="noopener">https://francemm.atlassian.net/browse/IA-98</a></li><li>Technologie | Mistral AI | Frontier AI in your hands<br><a href="https://mistral.ai/fr/technology/" target="_blank" rel="noopener">https://mistral.ai/fr/technology/</a></li><li>Title Not Found<br><a href="https://platform.openai.com/docs/guides/rate-limits/usage-tiers" target="_blank" rel="noopener">https://platform.openai.com/docs/guides/rate-limits/usage-tiers</a></li><li>Technologie | Mistral AI | Frontier AI in your hands<br><a href="https://mistral.ai/fr/technology/" target="_blank" rel="noopener">https://mistral.ai/fr/technology/</a></li><li>Pay As You Go—Buy Directly | Microsoft Azure<br><a href="https://azure.microsoft.com/en-us/pricing/purchase-options/pay-as-you-go" target="_blank" rel="noopener">https://azure.microsoft.com/en-us/pricing/purchase-options/pay-as-you-go</a></li><li>
Mistral Large now available on Azure 
<br><a href="https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/mistral-large-mistral-ai-s-flagship-llm-debuts-on-azure-ai/ba-p/4066996" target="_blank" rel="noopener">https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/mistral-large-mistral-ai-s-flagship-llm-debuts-on-azure-ai/ba-p/4066996</a></li><li>Azure AI | Mistral AI Large Language Models<br><a href="https://docs.mistral.ai/deployment/cloud/azure/" target="_blank" rel="noopener">https://docs.mistral.ai/deployment/cloud/azure/</a></li>




<ul>



</body>
</html>




