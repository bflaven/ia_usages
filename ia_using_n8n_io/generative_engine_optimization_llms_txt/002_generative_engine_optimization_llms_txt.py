#!/usr/bin/python
# -*- coding: utf-8 -*-

"""
[env]
# Conda Environment
conda create --name geo_llms_txt_poc python=3.9.13
conda info --envs
source activate geo_llms_txt_poc
conda deactivate

# if needed to remove
conda env remove -n [NAME_OF_THE_CONDA_ENVIRONMENT]
conda env remove -n geo_llms_txt_poc


# update conda 
conda update -n base -c defaults conda

# to export requirements
pip freeze > requirements.txt

# to install
pip install -r requirements.txt

pip install beautifulsoup4
pip install requests

python -m pip install beautifulsoup4
python -m pip install requests

# [path]
cd /Users/brunoflaven/Documents/03_git/ia_usages/ia_using_n8n_io/generative_engine_optimization_llms_txt


# LAUNCH the file
python 002_generative_engine_optimization_llms_txt.py

"""


import requests
from bs4 import BeautifulSoup

BASE_URL = "https://flaven.fr"
N_POSTS = 10
N_PAGES = 10

def get_soup(url):
    resp = requests.get(url, timeout=10)
    if resp.status_code == 200:
        return BeautifulSoup(resp.text, "html.parser")
    return None

def extract_posts(soups):
    posts = []
    for soup in soups:
        for article in soup.find_all("article"):
            h2 = article.find("h2")
            if h2 and h2.find("a"):
                title = h2.get_text(strip=True)
                link = h2.find("a")["href"]
                # Try to get description/first paragraph
                desc_elem = article.find("p")
                desc = desc_elem.get_text(strip=True) if desc_elem else ""
                posts.append((title, link, desc))
                if len(posts) >= N_POSTS:
                    return posts
    return posts

def extract_pages(soup):
    # Typical WordPress: Pages might be in menu or sidebar
    pages = []
    # Try menu
    nav = soup.find("nav")
    if nav:
        for a in nav.find_all("a", href=True):
            href = a['href']
            if '/page/' not in href and BASE_URL in href:  # Exclude blog pagers
                text = a.get_text(strip=True)
                pages.append((text, href))
    # Remove duplicates:
    seen = set()
    unique = []
    for t, h in pages:
        if h not in seen:
            unique.append((t, h))
            seen.add(h)
        if len(unique) >= N_PAGES:
            break
    return unique

def scrape_main():
    soups = []
    for i in range(1, N_PAGES + 1):
        page_url = BASE_URL if i == 1 else f"{BASE_URL}/page/{i}/"
        soup = get_soup(page_url)
        if soup:
            soups.append(soup)
        else:
            break

    posts = extract_posts(soups)[:N_POSTS]
    main_soup = soups[0] if soups else None
    pages = extract_pages(main_soup) if main_soup else []

    llms_txt = [
        "# Bruno Flaven's website",
        "",
        "> Professional blog of Bruno Flaven, currently working as AI Coordinator with over 20 years' experience. More at [flaven.fr](https://flaven.fr) or [LinkedIn](https://www.linkedin.com/in/brunoflaven).",
        "",
        "## Homepage",
        f"- [Homepage]({BASE_URL}): Bruno Flaven's personal and professional articles on AI, mobile, and digital product management.",
        "",
        "## Posts"
    ]
    for title, link, desc in posts:
        if desc:
            llms_txt.append(f"- [{title}]({link}): {desc}")
        else:
            llms_txt.append(f"- [{title}]({link})")
    llms_txt.append("")
    llms_txt.append("## Pages")
    for title, link in pages:
        llms_txt.append(f"- [{title}]({link})")
    llms_txt.append("")
    llms_txt.append("## Code")
    llms_txt.append("- Shared scripts, code samples, and project demos in the articles and blog posts.")
    llms_txt.append("")
    llms_txt.append("## Optional")
    llms_txt.append(f"- [LinkedIn profile](https://www.linkedin.com/in/brunoflaven)")
    llms_txt.append(f"- [Youtube](https://www.youtube.com/channel/UCnUBoVx9Yai3wirPBvNpNQw)")
    llms_txt.append(f"- [Github](https://github.com/bflaven/)")
    llms_txt.append(f"- [Amazon](https://amzn.to/2WQbRpA)")
    llms_txt.append(f"[comment]: # (Generated by Bruno for flaven.fr)")




    with open("llms_model_extended_flaven_fr.txt", "w", encoding="utf-8") as f:
        f.write('\n'.join(llms_txt))
    print("llms_model_extended_flaven_fr.txt file successfully created!")

if __name__ == "__main__":
    scrape_main()



